
## Pip install is for google colab
!pip install tifffile joblib tqdm tqdm_joblib


#Import things...May have to install a few things
import os
import time  # For timing
import numpy as np
import pandas as pd

from skimage import io
from skimage.measure import regionprops, label, ransac
from skimage.transform import AffineTransform, warp, resize
from skimage.filters import (threshold_otsu, threshold_local, gaussian)
from skimage.morphology import (remove_small_objects, binary_closing, disk)
from skimage.segmentation import clear_border
from scipy.ndimage import binary_fill_holes
from scipy.spatial import cKDTree
import tifffile as tiff

# For parallel + progress bar
from joblib import Parallel, delayed
from tqdm import tqdm
from tqdm_joblib import tqdm_joblib

##############################
# Functions for running
##############################

def normalize_image(image):
    """
    Normalize an image to the [0, 1] range for segmentation purposes.
    """
    return (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-10)

def flexible_segmentation(image, sigma=0.5, min_size=100, block_size=None, offset=10):
    """
    Perform segmentation using Gaussian blur, thresholding (global or local),
    and morphological cleaning.
    """
    blurred = gaussian(image, sigma=sigma, preserve_range=True)

    if block_size is None:
        # Global Otsu threshold
        thresh = threshold_otsu(blurred)
        binary_mask = blurred > thresh
    else:
        # Local threshold
        local_thresh = threshold_local(blurred, block_size=block_size, offset=offset)
        binary_mask = blurred > local_thresh

    filled_mask = binary_fill_holes(binary_mask)
    cleaned_mask = clear_border(filled_mask)
    final_mask = remove_small_objects(cleaned_mask, min_size=min_size)
    closed_mask = binary_closing(final_mask, disk(3))
    labeled_mask = label(closed_mask)
    return labeled_mask

def extract_centroids(labels):
    props = regionprops(labels)
    return np.array([p.centroid for p in props if p.label != 0])

def match_centroids(fixed_centroids, moving_centroids, distance_threshold=50):
    tree = cKDTree(moving_centroids)
    distances, indices = tree.query(fixed_centroids)
    valid = distances < distance_threshold
    return fixed_centroids[valid], moving_centroids[indices[valid]]

def estimate_transform_ransac(fixed_points, moving_points,
                              max_points=200,
                              residual_threshold=5,
                              max_trials=1000):
    n = len(fixed_points)
    if n > max_points:
        idx = np.random.choice(n, max_points, replace=False)
        fixed_sub = fixed_points[idx]
        moving_sub = moving_points[idx]
    else:
        fixed_sub = fixed_points
        moving_sub = moving_points

    model_robust, inliers_mask = ransac(
        (moving_sub, fixed_sub),  # (source, destination)
        AffineTransform,
        min_samples=3,
        residual_threshold=residual_threshold,
        max_trials=max_trials
    )
    return model_robust, inliers_mask

def apply_transform_with_padding(image, tform, output_shape, padding_value=0):
    """
    Apply the transformation with padding, ensuring shifted and edged areas are filled with padding.
    """
    transformed_image = warp(
        image,
        inverse_map=tform.inverse,
        output_shape=output_shape,
        mode='constant',
        cval=padding_value
    )
    return transformed_image

def resize_image(image, target_shape):
    return resize(
        image,
        target_shape,
        mode='reflect',
        anti_aliasing=True,
        preserve_range=True,
        order=3 #tweak order to a lower value if the process is too slow
    )

def save_registered_image_with_padding(output_dir, filename, image, original_dtype, padding_value=0):
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, filename)

    if np.issubdtype(original_dtype, np.integer):
        max_val = np.iinfo(original_dtype).max
        min_val = 0
        image_clipped = np.clip(image, min_val, max_val)
        image_final = image_clipped.astype(original_dtype)
    else:
        image_final = image.astype(original_dtype)

    tiff.imwrite(out_path, image_final)
    print(f"Saved registered image with padding to: {out_path}")



##############################
# Settings
##############################
base_dir = "/content/drive/My Drive/OPS_Nov2024/OriginalTIFF"
output_dir = os.path.join(base_dir, "Registered_Images_RANSAC_Flexible2") #idea was to store the registered images in to the same folder as a sub directory
os.makedirs(output_dir, exist_ok=True)

tile_list = range(440)      # e.g. 0..439
round_list = range(1, 9)    # e.g. 1..8

MAX_POINTS = 200
RANSAC_RESIDUAL_THRESH = 5
RANSAC_MAX_TRIALS = 1000

##############################
# Process One Tile
##############################
def process_one_tile(tile_num):
    tile_str = f"{tile_num:03d}"
    local_log = []

    # Load fixed image (Phenotype_DAPI was reference)
    fixed_filename = f"Phenotype_Tile-{tile_str}_c0_DAPI.tif"
    fixed_path = os.path.join(base_dir, fixed_filename)
    if not os.path.exists(fixed_path):
        print(f"[Tile {tile_str}] Missing Phenotype DAPI: {fixed_path}")
        return local_log

    fixed_img = io.imread(fixed_path)
    dtype_fixed = fixed_img.dtype

    # Segment fixed image
    fixed_norm = normalize_image(fixed_img)
    fixed_labels = flexible_segmentation(
        fixed_norm,
        sigma=0.5,
        min_size=100,
        block_size=None,  # set to a value if you want local thresholding
        offset=10
    )
    fixed_centroids = extract_centroids(fixed_labels)
    if len(fixed_centroids) < 3:
        print(f"[Tile {tile_str}] Not enough nuclei in FIXED after segmentation.")
        return local_log

    # For each round
    for round_num in round_list:
        round_str = f"ISS-Round-{round_num}"
        moving_filename = f"{round_str}_Tile-{tile_str}_c0_DAPI.tif"
        moving_path = os.path.join(base_dir, moving_filename)

        if not os.path.exists(moving_path):
            print(f"[Tile {tile_str}, Round {round_num}] Missing DAPI: {moving_path}")
            continue

        moving_img = io.imread(moving_path)
        dtype_moving = moving_img.dtype

        # Resize
        if fixed_img.shape != moving_img.shape:
            moving_img = resize_image(moving_img, fixed_img.shape)

        # Segment moving image
        moving_norm = normalize_image(moving_img)
        moving_labels = flexible_segmentation(
            moving_norm,
            sigma=0.5,
            min_size=100,
            block_size=None,
            offset=10
        )
        moving_centroids = extract_centroids(moving_labels)
        if len(moving_centroids) < 3:
            print(f"[Tile {tile_str}, Round {round_num}] Not enough nuclei in MOVING after segmentation.")
            continue

        # Match centroids
        fixed_matched, moving_matched = match_centroids(fixed_centroids, moving_centroids, distance_threshold=50)
        if len(fixed_matched) < 3:
            print(f"[Tile {tile_str}, Round {round_num}] Not enough matched points.")
            continue

        # RANSAC
        model_robust, inliers_mask = estimate_transform_ransac(
            fixed_matched,
            moving_matched,
            max_points=MAX_POINTS,
            residual_threshold=RANSAC_RESIDUAL_THRESH,
            max_trials=RANSAC_MAX_TRIALS
        )
        inliers_count = np.sum(inliers_mask)
        print(f"[Tile {tile_str}, Round {round_num}] RANSAC inliers: {inliers_count} / {len(inliers_mask)}")

        # Warp DAPI with padding
        registered_dapi_float = apply_transform_with_padding(moving_img, model_robust, fixed_img.shape)
        reg_dapi_filename = f"{round_str}_Tile-{tile_str}_c0_DAPI_registered_with_padding.tif"
        save_registered_image_with_padding(output_dir, reg_dapi_filename, registered_dapi_float, dtype_moving)

        # Log padding percentage
        padding_mask = create_padding_mask(moving_img, model_robust, fixed_img.shape)
        log_padding_percentage(padding_mask)

        # Log the registration
        local_log.append({
            "tile": tile_str,
            "round": round_num,
            "fixed_path": fixed_path,
            "moving_path": moving_path,
            "inliers_count": int(inliers_count)
        })

        # Warp additional channels with padding (if any)
        channel_info = {
            1: "G-Filter",
            2: "T-Filter",
            3: "A-Filter",
            4: "C-Filter"
        }
        for ch_idx, ch_name in channel_info.items():
            ch_filename = f"{round_str}_Tile-{tile_str}_c{ch_idx}_{ch_name}.tif"
            ch_path = os.path.join(base_dir, ch_filename)
            if not os.path.exists(ch_path):
                print(f"[Tile {tile_str}, Round {round_num}] Missing channel {ch_idx}: {ch_path}")
                continue

            ch_img = io.imread(ch_path)
            dtype_ch = ch_img.dtype
            if ch_img.shape != fixed_img.shape:
                ch_img = resize_image(ch_img, fixed_img.shape)

            registered_ch_float = apply_transform_with_padding(ch_img, model_robust, fixed_img.shape)
            out_ch_filename = f"{round_str}_Tile-{tile_str}_c{ch_idx}_{ch_name}_registered_with_padding.tif"
            save_registered_image_with_padding(output_dir, out_ch_filename, registered_ch_float, dtype_ch)

    return local_log

#################################################################################################################################################################
# Run in Parallel with Progress Bar & Timing (The prgress bar only works if you set n_jobs to > 1) didnt have a time to chunk the tiles but it may adress the issue
#################################################################################################################################################################
n_jobs = 1  # Think about if you have enough computation resources before trying a number greater than one

# Trying the progress Bar

start_time = time.time()
print("Starting registration pipeline...")

with tqdm_joblib(tqdm(desc="Tiles", total=len(tile_list))) as progress_bar:
    all_logs = Parallel(n_jobs=n_jobs)(
        delayed(process_one_tile)(tile_num) for tile_num in tile_list
    )

end_time = time.time()
print(f"Registration completed in {end_time - start_time:.2f} seconds.")

# Flatten logs
all_logs_flat = [row for partial_log in all_logs for row in partial_log]

# Save to CSV
df_log = pd.DataFrame(all_logs_flat)
log_csv = os.path.join(output_dir, "registration_ransac_flexible_log.csv")
df_log.to_csv(log_csv, index=False)
print(f"Registration log saved: {log_csv}")
